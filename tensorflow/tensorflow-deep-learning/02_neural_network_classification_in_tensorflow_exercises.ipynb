{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/john-james-ai/tensorflow-deep-learning/blob/main/02_neural_network_classification_in_tensorflow_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification in Tensorflow\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples, \n",
    "                    noise=0.03, \n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary created by a model predicting on X.\n",
    "    This function has been adapted from two phenomenal resources:\n",
    "    1. CS231n - https://cs231n.github.io/neural-networks-case-study/\n",
    "    2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n",
    "    \"\"\"\n",
    "    # Define the axis boundaries of the plot and create a meshgrid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Create X values (we're going to predict on all of these)\n",
    "    x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    y_pred = model.predict(x_in)\n",
    "\n",
    "    # Check for multi-class\n",
    "    if model.output_shape[-1] > 1: # checks the final dimension of the model's output shape, if this is > (greater than) 1, it's multi-class \n",
    "        print(\"doing multiclass classification...\")\n",
    "    # We have to reshape our predictions to get them ready for plotting\n",
    "        y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n",
    "    else:\n",
    "        print(\"doing binary classifcation...\")\n",
    "        y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play with neural networks in the [TensorFlow Playground](https://playground.tensorflow.org/) for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replicate the model pictured in the [TensorFlow Playground diagram](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6,6,6,6,6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true) below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model.\n",
    "![tensorflow playground example neural network](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png)\n",
    "*Try this network out for yourself on the [TensorFlow Playground website](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6,6,6,6,6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true). Hint: there are 5 hidden layers but the output layer isn't pictured, you'll have to decide what the output layer should be based on the input data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model\n",
    "model_2 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 3, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 4, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 5, ReLU activation\n",
    "  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model_2.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Check model summary\n",
    "model_2.summary()\n",
    "\n",
    "# Plot Decision Boundary\n",
    "plot_decision_boundary(model_2, X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a classification dataset using Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function, visualize it and then build a model to fit it at over 85% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_moons(n_samples, \n",
    "                    noise=0.03, \n",
    "                    random_state=42)\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model\n",
    "model_3 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 3, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 4, ReLU activation\n",
    "  tf.keras.layers.Dense(6, activation=tf.keras.activations.relu), # hidden layer 5, ReLU activation\n",
    "  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model_3.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Check model summary\n",
    "model_3.summary()\n",
    "\n",
    "# Plot Decision Boundary\n",
    "plot_decision_boundary(model_3, X,y)                    \n",
    "\n",
    "# Evaluate the model\n",
    "model_3.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the [classifcation tutorial in the TensorFlow documentation](https://www.tensorflow.org/tutorials/keras/classification) for ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_images(images, labels, class_names):\n",
    "    \"\"\"Plots multiple images and their class labels. Adapted from Tensorflow Keras Classification\n",
    "    Tutorial at https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "    Args:\n",
    "        images (list): List of normalized images.\n",
    "        labels (list): List of training labels corresponding to the images\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[labels[i]]).set_color('white')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data has already been sorted into training and test sets for us\n",
    "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "plot_multiple_images(images=train_data, labels=train_labels, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Recreate [TensorFlow's](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_float = train_data.astype('float')\n",
    "train_data_float[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    return tf.math.exp(X) / tf.reduce_sum(tf.math.exp(X))\n",
    "# softmax(train_data_float[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and normalize Data\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The following confusion matrix code is a remix of Scikit-Learn's \n",
    "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
    "# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n",
    "\n",
    "\n",
    "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): \n",
    "    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "\n",
    "    If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
    "    will be used.\n",
    "\n",
    "    Args:\n",
    "    y_true: Array of truth labels (must be same shape as y_pred).\n",
    "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
    "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "    figsize: Size of output figure (default=(10, 10)).\n",
    "    text_size: Size of output figure text (default=15).\n",
    "\n",
    "    Returns:\n",
    "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "\n",
    "    Example usage:\n",
    "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
    "                            y_pred=y_preds, # predicted labels\n",
    "                            classes=class_names, # array of class label names\n",
    "                            figsize=(15, 15),\n",
    "                            text_size=10)\n",
    "    \"\"\"  \n",
    "    # Create the confustion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "    n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "    # Plot the figure and make it pretty\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Are there a list of classes?\n",
    "    if classes:\n",
    "        labels = classes\n",
    "    else:\n",
    "        labels = np.arange(cm.shape[0])\n",
    "\n",
    "    # Label the axes\n",
    "    ax.set(title=\"Confusion Matrix\",\n",
    "            xlabel=\"Predicted label\",\n",
    "            ylabel=\"True label\",\n",
    "            xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "            yticks=np.arange(n_classes), \n",
    "            xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "            yticklabels=labels)\n",
    "\n",
    "    # Make x-axis labels appear on bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    # Set the threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "    # Plot the text on each cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                size=text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Set model hyperparameters\n",
    "epochs = 40\n",
    "\n",
    "# Create a model\n",
    "model_6 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)), # input layer flattened from 28x28 to 784.\n",
    "  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n",
    "  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n",
    "  tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax) # output layer, sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_6.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Create the learning rate callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))                \n",
    "\n",
    "# Fit the model\n",
    "history = model_6.fit(train_data, train_labels, epochs=epochs, validation_data=(test_data, test_labels), callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning rate decay curve\n",
    "lrs = 1e-3 * (10**(np.arange(40)/20))\n",
    "plt.semilogx(lrs, history.history[\"loss\"]) # want the x-axis to be log-scale\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Finding the ideal learning rate\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions and convert to labels.\n",
    "y_probs = model_6.predict(test_data)\n",
    "y_preds = y_probs.argmax(axis=1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "make_confusion_matrix(y_true=test_labels, y_pred=y_preds, classes=class_names, figsize=(15,15), text_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the `T-shirt` class with their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oksgPs-meGHj"
   },
   "source": [
    "## Extra curriculum 📖\n",
    "* Watch 3Blue1Brown's neural networks video 2: [*Gradient descent, how neural networks learn*](https://www.youtube.com/watch?v=IHZwWFHWa-w). After you're done, write 100 words about what you've learned.\n",
    "  * If you haven't already, watch video 1: [*But what is a Neural Network?*](https://youtu.be/aircAruvnKk). Note the activation function they talk about at the end.\n",
    "* Watch [MIT's introduction to deep learning lecture 1](https://youtu.be/njKP3FqW3Sk) (if you haven't already) to get an idea of the concepts behind using linear and non-linear functions.\n",
    "* Spend 1-hour reading [Michael Nielsen's Neural Networks and Deep Learning book](http://neuralnetworksanddeeplearning.com/index.html).\n",
    "* Read the [ML-Glossary documentation on activation functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html). Which one is your favourite?\n",
    "  * After you've read the ML-Glossary, see which activation functions are available in TensorFlow by searching \"tensorflow activation functions\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36b73152c2db178526287f34cbb63ec745c67c7bfa91505daf2a50c69f17c492"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
